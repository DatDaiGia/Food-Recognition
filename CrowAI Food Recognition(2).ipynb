{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"## Install required packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install numpy==1.17.0\n!pip install tensorflow==1.15.2\n!pip install keras==2.1.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!git clone https://www.github.com/matterport/Mask_RCNN.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.chdir(\"Mask_RCNN/\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -r requirements.txt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!python setup.py -q install","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall pycocotools -y\n!pip install -q git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import packages"},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT_DIR = \"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append(os.path.join(\".\", \"Mask_RCNN\"))\nsys.path.append(ROOT_DIR)\nimport re\nimport random\nimport pandas as pd\nimport numpy as np\nimport mrcnn.model as modellib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.lines as lines\nimport matplotlib\nimport math\nimport logging\nimport json\nimport itertools\nimport glob\nimport cv2\nfrom tqdm import tqdm\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools.coco import COCO\nfrom pycocotools import mask as maskUtils\nfrom mrcnn.model import log\nfrom mrcnn.config import Config\nfrom mrcnn import visualize\nfrom mrcnn import utils\nfrom collections import Counter, defaultdict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ROOT_DIR = os.path.abspath(\".\")\nROOT_DIR","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Constant variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_TRAIN_DIR = \"/kaggle/input/food-recognition/train/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_VAL_DIR = \"/kaggle/input/food-recognition/val/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/food-recognition/\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining DatasetClass and Config"},{"metadata":{"trusted":true},"cell_type":"code","source":"class FoodChallengeDataset(utils.Dataset):\n    def load_dataset(self, dataset_dir, load_small=False, return_coco=True):\n        self.load_small = load_small\n        \n        if self.load_small:\n            self.annotation_path = os.path.join(dataset_dir, \"annotations-small.json\")\n        else:\n            self.annotation_path = os.path.join(dataset_dir, \"annotations.json\")\n        \n        image_dir = os.path.join(dataset_dir, \"images\")\n        print(\"Annotation path\", self.annotation_path)\n        print(\"Image Dir\", image_dir)\n        \n        assert os.path.exists(self.annotation_path) and os.path.exists(image_dir)\n        \n        self.coco = COCO(self.annotation_path)\n        self.image_dir = image_dir\n        \n        class_ids = self.coco.getCatIds()\n        image_ids = list(self.coco.imgs.keys())\n        \n        #Register classes\n        for _class_id in class_ids:\n            self.add_class(\"crowdai_food_challenge\", _class_id, self.coco.loadCats(_class_id)[0][\"name\"])\n            \n        #Register images\n        for _img_id in image_ids:\n            assert os.path.exists(os.path.join(self.image_dir, self.coco.imgs[_img_id][\"file_name\"]))\n            self.add_image(\n                \"crowdai_food_challenge\", image_id=_img_id, \n                path=os.path.join(self.image_dir, self.coco.imgs[_img_id][\"file_name\"]),\n                width=self.coco.imgs[_img_id][\"width\"],\n                height=self.coco.imgs[_img_id][\"height\"],\n                annotations=self.coco.loadAnns(self.coco.getAnnIds(\n                                                                    imgIds=_img_id,\n                                                                    catIds=class_ids,\n                                                                    iscrowd=None\n                                                                   )\n                                              )\n            )\n        \n        if return_coco:\n            return self.coco\n        \n    def load_mask(self, image_id):\n        image_infor = self.image_info[image_id]\n        print(image_info[\"source\"])\n        assert image_info[\"source\"] == \"crowdai_food_challenge\"\n        \n        instance_masks = []\n        class_ids = []\n        annotations = self.image_info[image_id][\"annotations\"]\n        \n        for annotation in annotations:\n            class_id = self.map_source_class_id(\"crowdai_food_challenge.{}\".format(annotation[\"category_id\"]))\n            \n            if class_id:\n                m = self.annToMask(annotation, image_infor[\"height\"], image_infor[\"width\"])\n                \n                if m.max() < 1:\n                    continue\n                    \n                instance_masks.append(m)\n                class_ids.append(class_id)\n        \n        if class_ids:\n            mask = np.stack(instance_masks, axis=2)\n            class_ids = np.array(class_ids, dtype=np.uint32)\n            return mask, class_ids\n        else:\n            return super(FoodChallengeDataset, self).load_mask(image_id)\n        \n    def image_reference(self, image_id):\n        return \"crowai-food-challenge::{}\".format(image_id)\n    \n    def annToRLE(self, ann, height, width):\n        segm = ann[\"segmentation\"]\n        \n        if isinstance(segm, list):\n            rles = maskUtils.frPyObjects(segm, height, width)\n            rle = maskUtils.merge(rles)\n        elif isinstance(segm[\"count\"], list):\n            rle = maskUtils.frPyObjects(segm, height, width)\n        else:\n            rle = ann['segmentation']\n            \n        return rle\n    \n    def annToMask(self, ann, height, width):\n        rle = self.annToRLE(ann, height, width)\n        m = maskUtils.decode(rle)\n        return m","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train = FoodChallengeDataset()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train.load_dataset(dataset_dir=DATA_TRAIN_DIR, load_small=False)\ndataset_train.prepare()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_val = FoodChallengeDataset()\ndataset_val.load_dataset(dataset_dir=DATA_VAL_DIR, load_small=False, return_coco=True)\ndataset_val.prepare()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class FoodChallengeConfig(Config):\n    NAME = \"crowai-food-challenge\"\n    IMAGES_PER_GPU = 2\n    GPU_COUNT = 1\n    BACKBONE = 'resnet50'\n    NUM_CLASSES = 62 # n_classes + background\n    STEPS_PER_EPOCH = len(dataset_train.image_ids) // 2\n    VALIDATION_STEPS = len(dataset_val.image_ids) // 2\n    LEARNING_RATE = 0.001\n    IMAGE_MAX_DIM = 256\n    IMAGE_MIN_DIM = 256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = FoodChallengeConfig()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config.display()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Augment"},{"metadata":{"trusted":true},"cell_type":"code","source":"import imgaug as ia\nfrom imgaug import augmenters as iaa","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_AUG_SEQUENCE = None\nDATA_AUG_NAME_LOADED = None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_aug_geometric():\n    return iaa.OneOf([\n        iaa.Sequential([iaa.Fliplr(0.5), iaa.Flipud(0.2)]),\n        iaa.Crop(percent=(0.0, 0.1)),\n        iaa.Crop(percent=(0.1, 0.3)),\n        iaa.Crop(percent=(0.3, 0.5)),\n        iaa.CropAndPad(percent=(-0.05, 0.1),\n                       pad_cval=(0, 255),\n                       pad_mode='constant'),\n        iaa.Sequential([\n            iaa.Affine(\n                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n                rotate=(-45, 45),\n                shear=(-16, 16),\n                order=[0,1],\n                mode=\"constant\",\n                cval=(0, 255)\n            ),\n            iaa.Sometimes(0.3, iaa.Crop(percent=(0.3, 0.5)))\n        ])\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_aug_non_geometric():\n    return iaa.Sequential([\n        iaa.Sometimes(0.3, iaa.Multiply((0.5, 1.5), per_channel=0.5)),\n        iaa.Sometimes(0.3, iaa.GaussianBlur(sigma=(0, 3.0))),\n        iaa.Sometimes(0.2, iaa.Grayscale(alpha=(0, 1.0))),\n        iaa.Sometimes(0.3, iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)))\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_aug_all():\n    return iaa.Sequential([\n        iaa.Sometimes(0.5, load_aug_geometric()),\n        iaa.Sometimes(0.3, load_aug_non_geometric())\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_aug_all2():\n    def sometime(aug):\n        return iaa.Sometimes(0.5, aug)\n    \n    return iaa.Sequential([\n        iaa.Sequential([iaa.Fliplr(0.5), iaa.Flipud(0.2)]),\n        iaa.Crop(percent=(0.0, 0.1)),\n        iaa.Crop(percent=(0.1, 0.3)),\n        iaa.Crop(percent=(0.3, 0.5)),\n        iaa.CropAndPad(percent=(-0.05, 0.1),\n                       pad_cval=(0, 255),\n                       pad_mode='constant'),\n        iaa.Sequential([\n            iaa.Affine(\n                scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n                translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n                rotate=(-45, 45),\n                shear=(-16, 16),\n                order=[0,1],\n                mode=\"constant\",\n                cval=(0, 255)\n            ),\n            iaa.Sometimes(0.3, iaa.Crop(percent=(0.3, 0.5)))\n        ]),\n        iaa.SomeOf((0, 5), [\n            sometime(iaa.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))),\n            iaa.OneOf([\n                iaa.GaussianBlur(sigma=(0, 3.0)),\n                iaa.AverageBlur(k=(2, 7)),\n                iaa.MedianBlur(k=(3, 11))\n            ]),\n            iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n            iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n            iaa.SimplexNoiseAlpha(iaa.OneOf([\n               iaa.EdgeDetect(alpha=(0.5, 1.0)),\n               iaa.DirectedEdgeDetect(direction=(0.0, 1.0))\n            ])),\n            iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n            iaa.OneOf([\n                iaa.Dropout((0.01, 0.1), per_channel=0.5),\n                iaa.CoarseDropout((0.03, 0.15), size_percent=(\n                    0.02, 0.05), per_channel=0.2),\n            ]),\n            iaa.Invert(0.05, per_channel=True),\n            iaa.Add((-10, 10), per_channel=0.5),\n            iaa.AddToHueAndSaturation((-20, 20)),\n            iaa.OneOf([\n                iaa.Multiply(\n                            (0.5, 1.5), per_channel=0.5),\n                iaa.FrequencyNoiseAlpha(\n                    exponent=(-4, 0),\n                    first=iaa.Multiply(\n                        (0.5, 1.5), per_channel=True),\n                    second=iaa.ContrastNormalization(\n                        (0.5, 2.0))\n                )\n            ]),\n            iaa.ContrastNormalization((0.5, 2.0), per_channel=0.5),\n            iaa.Grayscale(alpha=(0.0, 1.0)),\n            sometime(iaa.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)),\n            sometime(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n            sometime(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n        ], random_order=True)\n    ], random_order=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_aug_support = {\n    \"aug_all\": load_aug_all,\n    \"aug_geo\": load_aug_geometric,\n    \"aug_non_geo\": load_aug_non_geometric,\n    \"aug_all_2\": load_aug_all2,\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_aug(aug_name=\"aug_all_2\"):\n    global DATA_AUG_NAME_LOADED\n    \n    if DATA_AUG_NAME_LOADED is None:\n        DATA_AUG_SEQUENCE = list_aug_support[aug_name]()\n        DATA_AUG_NAME_LOADED = aug_name\n        \n    return DATA_AUG_SEQUENCE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataset Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\n\nclass_counts = Counter()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for image_info in dataset_train.image_info:\n    ann = image_info[\"annotations\"]\n    \n    for i in ann:\n        class_counts[i[\"category_id\"]] += 1\n    \nclass_mapping = {i[\"id\"]: i[\"name\"] for i in dataset_train.class_info}\n\nclass_counts = pd.DataFrame(class_counts.most_common(), columns=[\"class\", \"count\"])\nclass_counts[\"class\"] = class_counts[\"class\"].apply(lambda x: class_mapping[x])\nplt.figure(figsize=(12, 12))\nplt.barh(class_counts['class'], class_counts['count'])\nplt.title('Counts of classes of objects');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'We have {class_counts.shape[0]} classes!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_id = random.choice(dataset_train.image_ids)\nimage = dataset_train.load_image(image_id)\nplt.imshow(image)\nmask, class_ids = dataset_train.load_mask(image_id)\nbbox = utils.extract_bboxes(mask)\n\nprint(\"image\", image_id, dataset_train.image_reference(image_id))\nlog(\"mask\", mask)\nlog(\"class_ids\", class_ids)\nlog(\"image\", image)\nlog(\"bbox\", bbox)\nvisualize.display_instances(image, bbox, mask, class_ids, dataset_train.class_names, figsize=(12, 12))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset_train.image_info[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_images = defaultdict(list)\n\nfor ind, image_info in enumerate(dataset_train.image_info):\n    ann = image_info[\"annotations\"]\n    \n    for i in ann:\n        class_images[i['category_id']].append(ind)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_ids = np.random.choice(dataset_train.image_ids, 4)\n\nfor class_id in np.random.choice(list(class_images.keys()), 10):\n    image_id = np.random.choice(class_images[class_id], 1)[0]\n    image = dataset_train.load_image(image_id)\n    mask, class_ids = dataset_train.load_mask(image_id)\n    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Bbox"},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, class_id in enumerate(np.random.choice(list(class_images.keys()), 10)):\n    image_id = np.random.choice(list(class_images[class_id]), 1)[0]\n    image = dataset_train.load_image(image_id)\n    mask, class_ids = dataset_train.load_mask(image_id)\n    bbox = utils.extract_bboxes(mask)\n    visualize.display_instances(image, bbox, mask, class_ids, dataset_train.class_names, figsize=(12, 12))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Anchors"},{"metadata":{"trusted":true},"cell_type":"code","source":"backbone_shape = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)\nanchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n                                         config.RPN_ANCHOR_RATIOS,\n                                         backbone_shape,\n                                         config.BACKBONE_STRIDES,\n                                         config.RPN_ANCHOR_STRIDE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_levels = len(backbone_shape)\nanchors_per_cell = len(config.RPN_ANCHOR_RATIOS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"levels\", num_levels)\nprint(\"scales\", config.RPN_ANCHOR_SCALES)\nprint(\"ratio\", config.RPN_ANCHOR_RATIOS)\nprint(\"anchors per cell\", anchors_per_cell)\nprint(\"number anchors\", anchors.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anchors_per_level = []\n\nfor l in range(num_levels):\n    num_cells = backbone_shape[l][0] * backbone_shape[l][1]\n    anchors_per_level.append(anchors_per_cell * num_cells // config.RPN_ANCHOR_STRIDE**2)\n    \n    print(\"Anchors per level \", l, anchors_per_level[l])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_id = np.random.choice(dataset_train.image_ids, 1)[0]\nimage, image_meta, _, _, _ = modellib.load_image_gt(dataset_train, config, image_id)\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.imshow(image)\n\nlevels = len(backbone_shape)\n\nfor level in range(levels):\n    colors = visualize.random_colors(levels)\n    level_start = sum(anchors_per_level[:level])\n    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n    print(\"Level {}. Anchors {:6} Feature map shape: {}\".format(level, level_anchors.shape[0], backbone_shape[level]))\n    \n    center_cell = backbone_shape[level] // 2\n    center_cell_index = (center_cell[0] * backbone_shape[level][1] + center_cell[1])\n    \n    level_center = center_cell_index * anchors_per_cell \n    center_anchor = anchors_per_cell * (\n        (center_cell[0] * backbone_shape[level][1] / config.RPN_ANCHOR_STRIDE**2) \\\n        + center_cell[1] / config.RPN_ANCHOR_STRIDE)\n    level_center = int(center_anchor)\n    \n    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n        y1, x1, y2, x2 = rect\n        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n                              edgecolor=(i+1)*np.array(colors[level]) / anchors_per_cell)\n        ax.add_patch(p)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ROI"},{"metadata":{"trusted":true},"cell_type":"code","source":"# random_rois = 2000\n# dataset.class_ids.astype(\"int\")\n\n# g = modellib.data_generator(\n#         dataset, config, shuffle=True, random_rois=random_rois, \n#         batch_size=4,\n#         detection_targets=True\n#     )\n\n# # Get Next Image\n# if random_rois:\n#     [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n#     [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n# else:\n#     [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n    \n# image_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\n\n# mrcnn_class_ids = mrcnn_class_ids[:,:,0]\n\n# b = 0\n\n# # Restore original image (reverse normalization)\n# sample_image = modellib.unmold_image(normalized_images[b], config)\n\n# # Compute anchor shifts.\n# indices = np.where(rpn_match[b] == 1)[0]\n# refined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\n\n# # Get list of positive anchors\n# positive_anchor_ids = np.where(rpn_match[b] == 1)[0]\n# negative_anchor_ids = np.where(rpn_match[b] == -1)[0]\n# neutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\n\n# # ROI breakdown by class\n# for c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n#     if n:\n#         print(\"{:23}: {}\".format(c[:20], n))\n\n# # Show positive anchors\n# visualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids], \n#                      refined_boxes=refined_anchors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# if random_rois:\n#     # Class aware bboxes\n#     bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n\n#     # Refined ROIs\n#     refined_rois = utils.apply_box_deltas(rois[b].astype(np.float32), bbox_specific[:,:4] * config.BBOX_STD_DEV)\n\n#     # Class aware masks\n#     mask_specific = mrcnn_mask[b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]]\n\n#     visualize.draw_rois(sample_image, rois[b], refined_rois, mask_specific, mrcnn_class_ids[b], dataset.class_names)\n    \n#     # Any repeated ROIs?\n#     rows = np.ascontiguousarray(rois[b]).view(np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1])))\n#     _, idx = np.unique(rows, return_index=True)\n    \n#     print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling with MaskRCNN"},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir pretrained","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PRETRAINED_MODEL_PATH = os.path.join(\"pretrained\", \"mask_rcnn_coco.h5\")\nLOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")\n\nif not os.path.exists(PRETRAINED_MODEL_PATH):\n    utils.download_trained_weights(PRETRAINED_MODEL_PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras.backend as K","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if K.backend() == \"tensorflow\":\n    K.common.image_dim_ordering()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=LOGS_DIRECTORY)\nmodel_path = PRETRAINED_MODEL_PATH\nmodel.load_weights(model_path, by_name=True, exclude=[\n    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"\n])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class_names = dataset_train.class_names\n# assert len(class_names)==62, \"Please check DatasetConfig\"\n# dataset_train.class_names","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\nfile_path = \"logs/best_weight.hdf5\"\n\ncheckpoint = ModelCheckpoint(filepath=file_path, mode=\"min\", verbose=1, monitor=\"val_acc\", save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config.LEARNING_RATE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import LearningRateScheduler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_schedule(epoch):\n    initial_lr = config.LEARNING_RATE\n    drop_every = 10\n    factor = 0.5\n    \n    lr = initial_lr * factor**(np.floor((1 + epoch)/drop_every))\n    \n    return lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training network\")\n\nmodel.train(dataset_train, dataset_val,\n            learning_rate=config.LEARNING_RATE,\n            epochs=45,\n            layers='heads', custom_callbacks=[checkpoint, LearningRateScheduler(lr_schedule)],\n            augmentation=load_aug())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm logs/crowai-food-challenge20200427T0506/mask_rcnn_crowai-food-challenge_0039.h5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.epoch = 45","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"class InferenceConfig(FoodChallengeConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    NUM_CLASSES = 62\n    IMAGE_MAX_DIM = 256\n    IMAGE_MIN_DIM = 256\n    NAME = \"Food\"\n    DETECTION_MIN_CONFIDENCE = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_config = InferenceConfig()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inference_config.display()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}